{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Multi-Agent RL Features Demo\n",
    "\n",
    "This notebook demonstrates state-of-the-art features for multi-agent reinforcement learning:\n",
    "\n",
    "1. **Attention-based Communication (TarMAC)** - Learned, targeted communication\n",
    "2. **Graph Neural Networks (GNN)** - Scalable agent coordination\n",
    "3. **Recurrent Policies (LSTM)** - Handling partial observability\n",
    "4. **Intrinsic Curiosity Module (ICM)** - Exploration bonus\n",
    "\n",
    "## Why These Features Matter\n",
    "\n",
    "- **TarMAC**: Enables agents to learn WHAT and WHEN to communicate, improving coordination\n",
    "- **GNN**: Scales to large numbers of agents by modeling relationships as graphs\n",
    "- **LSTM**: Essential for partially observable environments (real-world scenarios)\n",
    "- **ICM**: Improves exploration in sparse reward environments\n",
    "\n",
    "## Academic/Interview Relevance\n",
    "\n",
    "These are cutting-edge techniques from top-tier conferences (NeurIPS, ICML, ICLR)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import agents\n",
    "from src.marl.agents import (\n",
    "    DQNAgent,\n",
    "    AttentionDQNAgent,\n",
    "    GNNDQNAgent,\n",
    "    LSTMDQNAgent\n",
    ")\n",
    "\n",
    "# Import environments\n",
    "from src.marl.environments import MultiAgentGridWorld\n",
    "\n",
    "# Import utilities\n",
    "from src.marl.utils import attention, graph_networks, curiosity\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Attention-based Communication (TarMAC)\n",
    "\n",
    "### Key Concepts:\n",
    "- Agents generate **messages** from their observations\n",
    "- **Attention mechanism** determines which agents' messages to focus on\n",
    "- **Gated integration** controls how much to use communicated information\n",
    "\n",
    "### Reference:\n",
    "Das et al., \"TarMAC: Targeted Multi-Agent Communication\" (ICML 2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "env = MultiAgentGridWorld(\n",
    "    grid_size=(8, 8),\n",
    "    n_agents=3,\n",
    "    n_targets=3,\n",
    "    max_steps=50\n",
    ")\n",
    "\n",
    "# Create attention-based agents\n",
    "attention_agents = [\n",
    "    AttentionDQNAgent(\n",
    "        agent_id=i,\n",
    "        observation_space=env.observation_space,\n",
    "        action_space=env.action_space,\n",
    "        message_dim=32,\n",
    "        hidden_dim=64,\n",
    "        num_heads=4,\n",
    "        use_communication=True,\n",
    "        device=device\n",
    "    )\n",
    "    for i in range(3)\n",
    "]\n",
    "\n",
    "print(\"✓ Created 3 agents with TarMAC communication\")\n",
    "print(f\"  - Message dimension: 32\")\n",
    "print(f\"  - Number of attention heads: 4\")\n",
    "print(f\"  - Hidden dimension: 64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop with Communication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_communication(agents, env, n_episodes=100):\n",
    "    \"\"\"\n",
    "    Train agents with attention-based communication.\n",
    "    \"\"\"\n",
    "    episode_rewards = []\n",
    "    \n",
    "    for episode in tqdm(range(n_episodes), desc=\"Training with TarMAC\"):\n",
    "        observations, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        step = 0\n",
    "        \n",
    "        # Store messages from all agents\n",
    "        messages = {i: None for i in range(len(agents))}\n",
    "        \n",
    "        while not done and step < 50:\n",
    "            actions = {}\n",
    "            new_messages = {}\n",
    "            \n",
    "            # Each agent selects action and generates message\n",
    "            for i, agent in enumerate(agents):\n",
    "                # Collect messages from OTHER agents\n",
    "                other_messages = []\n",
    "                for j in range(len(agents)):\n",
    "                    if j != i and messages[j] is not None:\n",
    "                        other_messages.append(messages[j])\n",
    "                \n",
    "                # Stack messages if available\n",
    "                if len(other_messages) > 0:\n",
    "                    other_messages_tensor = torch.stack(other_messages)\n",
    "                else:\n",
    "                    other_messages_tensor = None\n",
    "                \n",
    "                # Get action and message\n",
    "                action, message = agent.get_action(\n",
    "                    observations[i],\n",
    "                    other_messages_tensor,\n",
    "                    training=True\n",
    "                )\n",
    "                \n",
    "                actions[i] = action\n",
    "                new_messages[i] = message\n",
    "            \n",
    "            # Update messages\n",
    "            messages = new_messages\n",
    "            \n",
    "            # Step environment\n",
    "            next_observations, rewards, terminated, truncated, _ = env.step(actions)\n",
    "            \n",
    "            # Store experiences and update agents\n",
    "            for i, agent in enumerate(agents):\n",
    "                agent.store_experience(\n",
    "                    observations[i],\n",
    "                    actions[i],\n",
    "                    rewards[i],\n",
    "                    next_observations[i],\n",
    "                    terminated[i]\n",
    "                )\n",
    "                agent.update()\n",
    "            \n",
    "            observations = next_observations\n",
    "            episode_reward += sum(rewards.values())\n",
    "            done = all(terminated.values())\n",
    "            step += 1\n",
    "        \n",
    "        episode_rewards.append(episode_reward)\n",
    "    \n",
    "    return episode_rewards\n",
    "\n",
    "# Train agents\n",
    "rewards = train_with_communication(attention_agents, env, n_episodes=100)\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(rewards, alpha=0.3)\n",
    "plt.plot(np.convolve(rewards, np.ones(10)/10, mode='valid'), linewidth=2)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('Training with Attention-based Communication (TarMAC)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✓ Training complete!\")\n",
    "print(f\"  Average reward (last 20 episodes): {np.mean(rewards[-20:]):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Attention Weights\n",
    "\n",
    "Let's see which agents each agent pays attention to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run one episode and collect attention weights\n",
    "observations, _ = env.reset()\n",
    "messages = {i: None for i in range(len(attention_agents))}\n",
    "attention_history = []\n",
    "\n",
    "for step in range(10):  # Run for 10 steps\n",
    "    actions = {}\n",
    "    new_messages = {}\n",
    "    step_attention = []\n",
    "    \n",
    "    for i, agent in enumerate(attention_agents):\n",
    "        other_messages = [messages[j] for j in range(len(attention_agents)) if j != i and messages[j] is not None]\n",
    "        \n",
    "        if len(other_messages) > 0:\n",
    "            other_messages_tensor = torch.stack(other_messages)\n",
    "        else:\n",
    "            other_messages_tensor = None\n",
    "        \n",
    "        action, message = agent.get_action(observations[i], other_messages_tensor, training=False)\n",
    "        actions[i] = action\n",
    "        new_messages[i] = message\n",
    "        \n",
    "        # Get attention weights\n",
    "        attn_weights = agent.get_attention_weights()\n",
    "        if attn_weights is not None:\n",
    "            step_attention.append(attn_weights.cpu().numpy())\n",
    "    \n",
    "    messages = new_messages\n",
    "    observations, _, terminated, truncated, _ = env.step(actions)\n",
    "    \n",
    "    if len(step_attention) > 0:\n",
    "        attention_history.append(step_attention)\n",
    "    \n",
    "    if all(terminated.values()):\n",
    "        break\n",
    "\n",
    "print(\"\\n✓ Collected attention weights during execution\")\n",
    "print(f\"  Steps recorded: {len(attention_history)}\")\n",
    "print(\"\\nAttention weights show which agents each agent focuses on during communication.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Graph Neural Networks (GNN)\n",
    "\n",
    "### Key Concepts:\n",
    "- Model agents as **nodes** in a graph\n",
    "- Communication as **message passing** on edges\n",
    "- **Dynamic graph construction** based on proximity\n",
    "- Scalable to 100+ agents\n",
    "\n",
    "### Reference:\n",
    "Velickovic et al., \"Graph Attention Networks\" (ICLR 2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create GNN-based agents\n",
    "gnn_agents = [\n",
    "    GNNDQNAgent(\n",
    "        agent_id=i,\n",
    "        observation_space=env.observation_space,\n",
    "        action_space=env.action_space,\n",
    "        n_agents=3,\n",
    "        gnn_type=\"gat\",  # Graph Attention Network\n",
    "        num_gnn_layers=2,\n",
    "        num_heads=4,\n",
    "        device=device\n",
    "    )\n",
    "    for i in range(3)\n",
    "]\n",
    "\n",
    "print(\"✓ Created 3 agents with Graph Neural Networks\")\n",
    "print(f\"  - GNN type: Graph Attention Network (GAT)\")\n",
    "print(f\"  - Number of GNN layers: 2\")\n",
    "print(f\"  - Number of attention heads: 4\")\n",
    "print(\"\\nGNNs enable scalable communication by modeling agent relationships as graphs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstrating GNN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show GNN architecture\n",
    "print(\"\\nGNN Q-Network Architecture:\")\n",
    "print(\"=\"*50)\n",
    "print(gnn_agents[0].q_network)\n",
    "print(\"=\"*50)\n",
    "print(\"\\nKey Components:\")\n",
    "print(\"1. Observation Encoder - Processes individual observations\")\n",
    "print(\"2. GNN Layers - Message passing between agents\")\n",
    "print(\"3. Q-Value Head - Outputs Q-values for actions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Recurrent Policies (LSTM)\n",
    "\n",
    "### Key Concepts:\n",
    "- **Memory** of past observations\n",
    "- Handle **partial observability** (POMDPs)\n",
    "- **Temporal reasoning** over sequences\n",
    "\n",
    "### Reference:\n",
    "Hausknecht & Stone, \"Deep Recurrent Q-Learning for Partially Observable MDPs\" (AAAI 2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LSTM-based agents\n",
    "lstm_agents = [\n",
    "    LSTMDQNAgent(\n",
    "        agent_id=i,\n",
    "        observation_space=env.observation_space,\n",
    "        action_space=env.action_space,\n",
    "        lstm_hidden_dim=64,\n",
    "        num_lstm_layers=1,\n",
    "        sequence_length=8,\n",
    "        device=device\n",
    "    )\n",
    "    for i in range(3)\n",
    "]\n",
    "\n",
    "print(\"✓ Created 3 agents with LSTM memory\")\n",
    "print(f\"  - LSTM hidden dimension: 64\")\n",
    "print(f\"  - Number of LSTM layers: 1\")\n",
    "print(f\"  - Sequence length for training: 8\")\n",
    "print(\"\\nLSTM enables agents to remember past observations and make better decisions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Intrinsic Curiosity Module (ICM)\n",
    "\n",
    "### Key Concepts:\n",
    "- **Intrinsic reward** based on prediction error\n",
    "- Encourages exploration of novel states\n",
    "- Learns task-relevant **features** automatically\n",
    "- Combines **inverse dynamics** (predict action) and **forward dynamics** (predict next state)\n",
    "\n",
    "### Reference:\n",
    "Pathak et al., \"Curiosity-driven Exploration by Self-supervised Prediction\" (ICML 2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ICM module\n",
    "icm_module = curiosity.IntrinsicCuriosityModule(\n",
    "    obs_dim=env.observation_space.shape[0],\n",
    "    action_dim=env.action_space.n,\n",
    "    feature_dim=32,\n",
    "    hidden_dim=64,\n",
    "    beta=0.2,\n",
    "    eta=0.5\n",
    ").to(device)\n",
    "\n",
    "print(\"✓ Created Intrinsic Curiosity Module\")\n",
    "print(f\"  - Feature dimension: 32\")\n",
    "print(f\"  - Beta (inverse model weight): 0.2\")\n",
    "print(f\"  - Eta (intrinsic reward scale): 0.5\")\n",
    "print(\"\\nICM Architecture:\")\n",
    "print(\"1. Feature Encoder - Learns task-relevant representations\")\n",
    "print(\"2. Inverse Model - Predicts action from state transition\")\n",
    "print(\"3. Forward Model - Predicts next state features (error = curiosity)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstrating Intrinsic Rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect intrinsic rewards during exploration\n",
    "observations, _ = env.reset()\n",
    "intrinsic_rewards_history = []\n",
    "\n",
    "for step in range(20):\n",
    "    # Random actions for exploration\n",
    "    actions = {i: env.action_space.sample() for i in range(3)}\n",
    "    next_observations, rewards, terminated, truncated, _ = env.step(actions)\n",
    "    \n",
    "    # Compute intrinsic rewards for each agent\n",
    "    step_intrinsic_rewards = []\n",
    "    for i in range(3):\n",
    "        obs_tensor = torch.FloatTensor(observations[i]).unsqueeze(0).to(device)\n",
    "        next_obs_tensor = torch.FloatTensor(next_observations[i]).unsqueeze(0).to(device)\n",
    "        action_tensor = torch.LongTensor([actions[i]]).to(device)\n",
    "        \n",
    "        intrinsic_reward, losses = icm_module(obs_tensor, next_obs_tensor, action_tensor)\n",
    "        step_intrinsic_rewards.append(intrinsic_reward.item())\n",
    "    \n",
    "    intrinsic_rewards_history.append(step_intrinsic_rewards)\n",
    "    observations = next_observations\n",
    "    \n",
    "    if all(terminated.values()):\n",
    "        break\n",
    "\n",
    "# Visualize intrinsic rewards\n",
    "intrinsic_rewards_history = np.array(intrinsic_rewards_history)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "for i in range(3):\n",
    "    plt.plot(intrinsic_rewards_history[:, i], label=f'Agent {i}', marker='o')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Intrinsic Reward')\n",
    "plt.title('Intrinsic Curiosity Rewards During Exploration')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Intrinsic rewards computed successfully!\")\n",
    "print(\"Higher rewards indicate more novel/surprising states.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparing All Approaches\n",
    "\n",
    "Let's compare the performance of different agent architectures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a simplified comparison - full training would take longer\n",
    "print(\"Feature Comparison:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Feature':<30} {'Baseline DQN':<15} {'TarMAC':<15} {'GNN':<15} {'LSTM':<15}\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Communication':<30} {'No':<15} {'Yes (Attn)':<15} {'Yes (Graph)':<15} {'No':<15}\")\n",
    "print(f\"{'Memory':<30} {'No':<15} {'No':<15} {'No':<15} {'Yes (LSTM)':<15}\")\n",
    "print(f\"{'Scalability (# agents)':<30} {'<10':<15} {'<20':<15} {'100+':<15} {'<10':<15}\")\n",
    "print(f\"{'Partial Observability':<30} {'Poor':<15} {'Poor':<15} {'Poor':<15} {'Good':<15}\")\n",
    "print(f\"{'Parameters':<30} {'Low':<15} {'Medium':<15} {'Medium':<15} {'Medium':<15}\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nKey Insights:\")\n",
    "print(\"• TarMAC: Best for tasks requiring selective communication\")\n",
    "print(\"• GNN: Best for large-scale multi-agent systems\")\n",
    "print(\"• LSTM: Best for partially observable environments\")\n",
    "print(\"• ICM: Useful as add-on for sparse reward environments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Implementation Highlights for Interviews\n",
    "\n",
    "### Key Technical Points to Discuss:\n",
    "\n",
    "#### 1. **Attention Mechanism**\n",
    "```python\n",
    "# Multi-head attention for communication\n",
    "Q = self.q_proj(query)  # What I'm looking for\n",
    "K = self.k_proj(key)    # What others are broadcasting\n",
    "V = self.v_proj(value)  # The actual message content\n",
    "\n",
    "attention_weights = softmax(Q @ K.T / sqrt(d_k))\n",
    "output = attention_weights @ V\n",
    "```\n",
    "\n",
    "#### 2. **Graph Neural Networks**\n",
    "```python\n",
    "# Message passing on agent graph\n",
    "for layer in gnn_layers:\n",
    "    # Aggregate neighbor features\n",
    "    messages = adjacency @ node_features\n",
    "    # Update node features\n",
    "    node_features = update_fn(messages)\n",
    "```\n",
    "\n",
    "#### 3. **Intrinsic Curiosity**\n",
    "```python\n",
    "# Forward model prediction error as curiosity\n",
    "predicted_next_state = forward_model(state, action)\n",
    "intrinsic_reward = ||predicted_next_state - actual_next_state||^2\n",
    "total_reward = extrinsic_reward + eta * intrinsic_reward\n",
    "```\n",
    "\n",
    "#### 4. **LSTM for Memory**\n",
    "```python\n",
    "# Maintain hidden state across steps\n",
    "h_t, c_t = lstm(observation_t, (h_{t-1}, c_{t-1}))\n",
    "q_values = q_network(h_t)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Research Papers and Citations\n",
    "\n",
    "### Implemented Features:\n",
    "\n",
    "1. **TarMAC (Targeted Multi-Agent Communication)**\n",
    "   - Das et al., ICML 2019\n",
    "   - \"TarMAC: Targeted Multi-Agent Communication\"\n",
    "\n",
    "2. **Graph Attention Networks**\n",
    "   - Veličković et al., ICLR 2018\n",
    "   - \"Graph Attention Networks\"\n",
    "\n",
    "3. **Intrinsic Curiosity Module**\n",
    "   - Pathak et al., ICML 2017\n",
    "   - \"Curiosity-driven Exploration by Self-supervised Prediction\"\n",
    "\n",
    "4. **Deep Recurrent Q-Learning**\n",
    "   - Hausknecht & Stone, AAAI 2015\n",
    "   - \"Deep Recurrent Q-Learning for Partially Observable MDPs\"\n",
    "\n",
    "5. **Random Network Distillation**\n",
    "   - Burda et al., ICLR 2019\n",
    "   - \"Exploration by Random Network Distillation\"\n",
    "\n",
    "### Additional References:\n",
    "- QMIX (Rashid et al., ICML 2018)\n",
    "- CommNet (Sukhbaatar et al., NeurIPS 2016)\n",
    "- MADDPG (Lowe et al., NeurIPS 2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated four cutting-edge features for multi-agent RL:\n",
    "\n",
    "✅ **Attention-based Communication (TarMAC)** - Learned selective communication\n",
    "\n",
    "✅ **Graph Neural Networks** - Scalable coordination for many agents\n",
    "\n",
    "✅ **Recurrent Policies (LSTM)** - Memory for partial observability\n",
    "\n",
    "✅ **Intrinsic Curiosity Module** - Exploration in sparse reward environments\n",
    "\n",
    "### Next Steps:\n",
    "1. Train agents for more episodes to see full convergence\n",
    "2. Try different hyperparameters\n",
    "3. Test on other environments (Cooperative Navigation, Predator-Prey)\n",
    "4. Combine features (e.g., GNN + ICM)\n",
    "5. Implement additional features (Meta-learning, Hierarchical policies)\n",
    "\n",
    "### For Your Project/Interview:\n",
    "- Understand the **mathematical foundations** of each technique\n",
    "- Be able to explain **when and why** to use each approach\n",
    "- Discuss **trade-offs** (computation, scalability, performance)\n",
    "- Mention relevant **research papers** and cite properly"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
